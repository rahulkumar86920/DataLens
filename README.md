# DataLens â€“ Smart Scraping Dashboard

![DataLens Banner](https://img.shields.io/badge/DataLens-Smart_Scraping-blue?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Production_Ready-success?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)

> **Enterprise-grade web scraping dashboard with Google OAuth, real-time data fetching, and modern React UI**

## ğŸ¯ Project Overview

DataLens is a full-stack web application that scrapes real-time data from Hacker News, stores it in MongoDB, and displays it in a beautiful React dashboard. Built with production-level architecture, security best practices, and complete documentation.

### âœ¨ Features

- ğŸ” **Google OAuth 2.0** - Secure authentication
- ğŸ•·ï¸ **Python Web Scraper** - Respects robots.txt with rate limiting
- ğŸ“Š **React Dashboard** - Modern, responsive UI with Tailwind CSS
- ğŸ—„ï¸ **MongoDB Storage** - Scalable NoSQL database
- ğŸ”„ **Real-time Refresh** - Manual scraping trigger
- ğŸ“± **Mobile Responsive** - Works on all devices
- ğŸš€ **Production Ready** - Deployment guides included

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Frontend** | React, Tailwind CSS, Context API |
| **Backend** | Node.js, Express, JWT |
| **Scraper** | Python, BeautifulSoup, Requests |
| **Database** | MongoDB Atlas |
| **Auth** | Google OAuth 2.0 |
| **Deployment** | Vercel (Frontend), Render (Backend) |

---

## ğŸ“ Project Structure

```
datalens/
â”œâ”€â”€ frontend/                 # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ pages/           # Page components
â”‚   â”‚   â”œâ”€â”€ services/        # API integration
â”‚   â”‚   â”œâ”€â”€ context/         # Global state
â”‚   â”‚   â””â”€â”€ utils/           # Helper functions
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/                  # Express API server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/          # Configuration files
â”‚   â”‚   â”œâ”€â”€ controllers/     # Request handlers
â”‚   â”‚   â”œâ”€â”€ middleware/      # Custom middleware
â”‚   â”‚   â”œâ”€â”€ models/          # MongoDB schemas
â”‚   â”‚   â”œâ”€â”€ routes/          # API routes
â”‚   â”‚   â”œâ”€â”€ utils/           # Utilities
â”‚   â”‚   â””â”€â”€ server.js        # Entry point
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ scraper/                  # Python scraper
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ scrapers/        # Scraping logic
â”‚   â”‚   â”œâ”€â”€ utils/           # Helper functions
â”‚   â”‚   â””â”€â”€ main.py          # Entry point
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md                 # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** v16+ and npm
- **Python** 3.8+
- **MongoDB Atlas** account (free tier)
- **Google Cloud Console** account

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/datalens.git
cd datalens
```

### 2ï¸âƒ£ Backend Setup

```bash
cd backend
npm install

# Create .env file
cp .env.example .env
```

**backend/.env**
```env
# Server
PORT=5000
NODE_ENV=development
FRONTEND_URL=http://localhost:3000

# MongoDB
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/datalens

# JWT
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production

# Google OAuth
GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-google-client-secret

# Logging
LOG_LEVEL=info
```

### 3ï¸âƒ£ Frontend Setup

```bash
cd ../frontend
npm install

# Create .env file
cp .env.example .env
```

**frontend/.env**
```env
REACT_APP_API_URL=http://localhost:5000
REACT_APP_GOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com
```

### 4ï¸âƒ£ Scraper Setup

```bash
cd ../scraper
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt

# Create .env file
cp .env.example .env
```

**scraper/.env**
```env
BACKEND_API_URL=http://localhost:5000
```

---

## ğŸ”‘ Google OAuth Setup

### Step-by-Step Guide

1. **Go to [Google Cloud Console](https://console.cloud.google.com/)**

2. **Create New Project**
   - Click "Select a project" â†’ "New Project"
   - Name: `DataLens`
   - Click "Create"

3. **Enable Google+ API**
   - Go to "APIs & Services" â†’ "Library"
   - Search "Google+ API"
   - Click "Enable"

4. **Configure OAuth Consent Screen**
   - Go to "APIs & Services" â†’ "OAuth consent screen"
   - User Type: External
   - Fill in:
     - App name: `DataLens`
     - User support email: `your-email@gmail.com`
     - Developer contact: `your-email@gmail.com`
   - Add scopes: `email`, `profile`, `openid`
   - Add test users (your email)
   - Click "Save and Continue"

5. **Create OAuth 2.0 Credentials**
   - Go to "Credentials" â†’ "Create Credentials" â†’ "OAuth client ID"
   - Application type: **Web application**
   - Name: `DataLens Web Client`
   - Authorized JavaScript origins:
     ```
     http://localhost:3000
     https://yourdomain.vercel.app
     ```
   - Authorized redirect URIs:
     ```
     http://localhost:3000
     https://yourdomain.vercel.app
     ```
   - Click "Create"

6. **Copy Credentials**
   - Copy **Client ID** â†’ Paste in `.env` files
   - Copy **Client Secret** â†’ Paste in backend `.env`

---

## ğŸ—„ï¸ MongoDB Atlas Setup

1. **Create Account** at [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)

2. **Create Cluster**
   - Choose FREE tier (M0)
   - Select region closest to you
   - Cluster name: `DataLens`

3. **Configure Database Access**
   - Go to "Database Access"
   - Add New Database User
   - Username: `datalens`
   - Password: Generate secure password
   - Role: `Read and write to any database`

4. **Configure Network Access**
   - Go to "Network Access"
   - Add IP Address
   - For development: `0.0.0.0/0` (Allow from anywhere)
   - For production: Add your server IPs

5. **Get Connection String**
   - Go to "Databases" â†’ "Connect"
   - Choose "Connect your application"
   - Copy connection string
   - Replace `<password>` with your password
   - Paste in backend `.env` as `MONGODB_URI`

---

## â–¶ï¸ Running the Application

### Development Mode

**Terminal 1: Backend**
```bash
cd backend
npm run dev
# Server runs on http://localhost:5000
```

**Terminal 2: Frontend**
```bash
cd frontend
npm start
# App runs on http://localhost:3000
```

**Terminal 3: Scraper (Manual)**
```bash
cd scraper
source venv/bin/activate  # Windows: venv\Scripts\activate
python src/main.py
```

### Production Mode

**Backend**
```bash
cd backend
npm start
```

**Frontend**
```bash
cd frontend
npm run build
serve -s build
```

---

## ğŸ“¡ API Documentation

### Authentication Endpoints

#### POST `/api/auth/google`
Login with Google OAuth

**Request:**
```json
{
  "idToken": "google-id-token-here"
}
```

**Response:**
```json
{
  "success": true,
  "message": "Login successful",
  "token": "jwt-token-here",
  "user": {
    "id": "user-id",
    "name": "John Doe",
    "email": "john@example.com",
    "picture": "profile-url"
  }
}
```

#### GET `/api/auth/verify`
Verify JWT token (requires authentication)

**Headers:**
```
Authorization: Bearer <jwt-token>
```

**Response:**
```json
{
  "success": true,
  "user": {
    "userId": "user-id",
    "email": "john@example.com",
    "name": "John Doe"
  }
}
```

### Data Endpoints

#### GET `/api/data`
Get all scraped data (requires authentication)

**Query Parameters:**
- `page` (optional): Page number (default: 1)
- `limit` (optional): Items per page (default: 30)

**Response:**
```json
{
  "success": true,
  "data": [
    {
      "_id": "item-id",
      "rank": 1,
      "title": "Story Title",
      "link": "https://example.com",
      "points": 250,
      "comments": 42,
      "scrapedAt": "2026-01-02T10:30:00.000Z"
    }
  ],
  "pagination": {
    "currentPage": 1,
    "totalPages": 2,
    "totalItems": 50,
    "itemsPerPage": 30
  },
  "lastScrapedAt": "2026-01-02T10:30:00.000Z"
}
```

#### POST `/api/scrape/trigger`
Trigger manual scraping (requires authentication)

**Response:**
```json
{
  "success": true,
  "message": "Scraping completed successfully",
  "itemsScraped": 30
}
```

#### GET `/api/data/stats`
Get dashboard statistics (requires authentication)

**Response:**
```json
{
  "success": true,
  "stats": {
    "totalItems": 30,
    "lastScrapedAt": "2026-01-02T10:30:00.000Z",
    "averagePoints": 156,
    "totalPoints": 4680
  }
}
```

#### POST `/api/scrape`
Receive scraped data (called by Python scraper)

**Request:**
```json
{
  "items": [
    {
      "rank": 1,
      "title": "Story Title",
      "link": "https://example.com",
      "points": 250,
      "comments": 42,
      "scraped_at": "2026-01-02 10:30:00"
    }
  ]
}
```

---

## ğŸš¢ Deployment

### Frontend Deployment (Vercel)

1. **Push code to GitHub**
```bash
git add .
git commit -m "Initial commit"
git push origin main
```

2. **Deploy to Vercel**
   - Go to [Vercel](https://vercel.com)
   - Click "Import Project"
   - Select your GitHub repository
   - Set root directory: `frontend`
   - Add environment variables:
     ```
     REACT_APP_API_URL=https://your-backend.onrender.com
     REACT_APP_GOOGLE_CLIENT_ID=your-client-id
     ```
   - Click "Deploy"

3. **Update Google OAuth**
   - Add Vercel URL to authorized origins in Google Console

### Backend Deployment (Render)

1. **Create New Web Service** on [Render](https://render.com)
   - Connect GitHub repository
   - Name: `datalens-backend`
   - Root Directory: `backend`
   - Build Command: `npm install`
   - Start Command: `npm start`

2. **Add Environment Variables**
   ```
   PORT=5000
   NODE_ENV=production
   FRONTEND_URL=https://your-app.vercel.app
   MONGODB_URI=your-mongodb-connection-string
   JWT_SECRET=your-production-jwt-secret
   GOOGLE_CLIENT_ID=your-client-id
   GOOGLE_CLIENT_SECRET=your-client-secret
   ```

3. **Deploy**
   - Click "Create Web Service"
   - Copy the Render URL

### Scraper Deployment

**Option 1: Scheduled Cron Job (Recommended)**
- Use Render Cron Jobs or GitHub Actions
- Schedule: Every hour

**Option 2: Manual Trigger**
- Deploy scraper as separate service
- Trigger via API endpoint from dashboard

---

## ğŸ”’ Security Best Practices

âœ… **Implemented:**
- JWT authentication with secure tokens
- Google OAuth for trusted identity
- Helmet.js for security headers
- CORS configured properly
- Input validation and sanitization
- Rate limiting on scraper
- Environment variables for secrets
- MongoDB connection encryption

âš ï¸ **Additional Recommendations:**
- Enable 2FA on all accounts
- Use strong JWT secrets (32+ characters)
- Implement rate limiting on API endpoints
- Set up SSL certificates (HTTPS)
- Regular dependency updates
- Monitor logs for suspicious activity

---

## ğŸ§ª Testing

### Manual Testing Checklist

- [ ] Google OAuth login works
- [ ] Dashboard loads after login
- [ ] Data displays correctly
- [ ] Refresh button triggers scraping
- [ ] Toast notifications appear
- [ ] Mobile responsive design
- [ ] Logout functionality
- [ ] API error handling

### API Testing with Postman

Import the following collection:

```json
{
  "info": { "name": "DataLens API" },
  "item": [
    {
      "name": "Health Check",
      "request": {
        "method": "GET",
        "url": "{{base_url}}/health"
      }
    },
    {
      "name": "Get Data",
      "request": {
        "method": "GET",
        "url": "{{base_url}}/api/data",
        "header": [
          {
            "key": "Authorization",
            "value": "Bearer {{token}}"
          }
        ]
      }
    }
  ]
}
```

---

## ğŸ“ Environment Variables Reference

### Backend (.env)

| Variable | Description | Example |
|----------|-------------|---------|
| `PORT` | Server port | `5000` |
| `NODE_ENV` | Environment | `development` or `production` |
| `FRONTEND_URL` | Frontend URL | `http://localhost:3000` |
| `MONGODB_URI` | MongoDB connection | `mongodb+srv://...` |
| `JWT_SECRET` | JWT signing key | `super-secret-key` |
| `GOOGLE_CLIENT_ID` | OAuth Client ID | `123-abc.apps.googleusercontent.com` |
| `GOOGLE_CLIENT_SECRET` | OAuth Secret | `GOCSPX-abc123` |

### Frontend (.env)

| Variable | Description | Example |
|----------|-------------|---------|
| `REACT_APP_API_URL` | Backend API URL | `http://localhost:5000` |
| `REACT_APP_GOOGLE_CLIENT_ID` | OAuth Client ID | `123-abc.apps.googleusercontent.com` |

### Scraper (.env)

| Variable | Description | Example |
|----------|-------------|---------|
| `BACKEND_API_URL` | Backend API URL | `http://localhost:5000` |

---

## ğŸ› Troubleshooting

### Common Issues

**Issue: Google OAuth not working**
- âœ… Check Client ID matches in both frontend and Google Console
- âœ… Verify redirect URIs are correctly configured
- âœ… Ensure Google+ API is enabled

**Issue: MongoDB connection failed**
- âœ… Check connection string format
- âœ… Verify network access allows your IP
- âœ… Confirm database user credentials

**Issue: Scraper fails**
- âœ… Verify backend API is running
- âœ… Check Python dependencies installed
- âœ… Ensure robots.txt is accessible

**Issue: CORS errors**
- âœ… Verify FRONTEND_URL in backend .env
- âœ… Check CORS configuration in server.js

---

## ğŸ“š Additional Resources

- [React Documentation](https://react.dev/)
- [Express.js Guide](https://expressjs.com/)
- [MongoDB Atlas Docs](https://www.mongodb.com/docs/atlas/)
- [Google OAuth Documentation](https://developers.google.com/identity/protocols/oauth2)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

---

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)
- Email: your.email@example.com

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Hacker News for providing public data
- Anthropic Claude for development assistance
- Open source community for amazing tools

---

## ğŸ¯ Roadmap

- [ ] Scheduled automatic scraping
- [ ] Multiple data sources
- [ ] Data export (CSV, JSON)
- [ ] Advanced analytics dashboard
- [ ] User preferences and saved searches
- [ ] Real-time WebSocket updates
- [ ] Dockerization
- [ ] CI/CD pipeline

---

**â­ If this project helped you, please consider giving it a star!**